{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import selfies as sf\n",
    "device = \"cpu\"\n",
    "from chemistry_vae import VAEDecoder, VAEEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smiles_encodings_for_dataset(file_path):\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    smiles_list = np.asanyarray(df.smiles)\n",
    "\n",
    "    smiles_alphabet = list(set(''.join(smiles_list)))\n",
    "    smiles_alphabet.append(' ')  # for padding\n",
    "    largest_smiles_len = len(max(smiles_list, key=len))\n",
    "\n",
    "    return smiles_list, smiles_alphabet, largest_smiles_len\n",
    "\n",
    "def get_selfies_encodings_for_dataset(smiles_list, largest_smiles_len):\n",
    "\n",
    "    print(\"Largest smiles len\", largest_smiles_len)\n",
    "    print('--> Translating SMILES to SELFIES...')\n",
    "    selfies_list = list(map(sf.encoder, smiles_list))\n",
    "    f = open(\"datasets/0SelectedSMILES_QM9.sf.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(selfies_list))\n",
    "    all_selfies_symbols = sf.get_alphabet_from_selfies(selfies_list)\n",
    "    all_selfies_symbols.add('[nop]')\n",
    "    selfies_alphabet = list(all_selfies_symbols)\n",
    "\n",
    "    largest_selfies_len = max(sf.len_selfies(s) for s in selfies_list)\n",
    "    print(\"Largest selfies len\", largest_selfies_len)\n",
    "\n",
    "    print('Finished translating SMILES to SELFIES.')\n",
    "    return selfies_list, selfies_alphabet, largest_selfies_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def selfies_to_hot(selfie, largest_selfie_len, alphabet):\n",
    "    \"\"\"Go from a single selfies string to a one-hot encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    symbol_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "\n",
    "    # pad with [nop]\n",
    "    selfie += '[nop]' * (largest_selfie_len - sf.len_selfies(selfie))\n",
    "\n",
    "    # integer encode\n",
    "    symbol_list = sf.split_selfies(selfie)\n",
    "    integer_encoded = [symbol_to_int[symbol] for symbol in symbol_list]\n",
    "\n",
    "    # one hot-encode the integer encoded selfie\n",
    "    onehot_encoded = list()\n",
    "    for index in integer_encoded:\n",
    "        letter = [0] * len(alphabet)\n",
    "        letter[index] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "\n",
    "    return integer_encoded, np.array(onehot_encoded)\n",
    "\n",
    "\n",
    "def multiple_selfies_to_hot(selfies_list, largest_molecule_len, alphabet):\n",
    "    \"\"\"Convert a list of selfies strings to a one-hot encoding\n",
    "    \"\"\"\n",
    "\n",
    "    hot_list = []\n",
    "    for s in selfies_list:\n",
    "        _, onehot_encoded = selfies_to_hot(s, largest_molecule_len, alphabet)\n",
    "        hot_list.append(onehot_encoded)\n",
    "    return np.array(hot_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_models(epoch):\n",
    "    print(\"loading models\")\n",
    "    out_dir = './saved_models/{}'.format(epoch)\n",
    "    encoder = torch.load('{}/E'.format(out_dir), map_location=torch.device('cpu'))\n",
    "    encoder.eval()\n",
    "    decoder = torch.load('{}/D'.format(out_dir), map_location=torch.device('cpu'))\n",
    "    decoder.eval()\n",
    "    return encoder, decoder\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest smiles len 22\n",
      "--> Translating SMILES to SELFIES...\n",
      "Largest selfies len 21\n",
      "Finished translating SMILES to SELFIES.\n",
      "loading models\n"
     ]
    }
   ],
   "source": [
    "# get all the inputs\n",
    "smiles_list, smiles_alphabet, largest_smiles_len = get_smiles_encodings_for_dataset(\"datasets/0SelectedSMILES_QM9.txt\")\n",
    "selfies_list, selfies_alphabet, largest_selfies_len = get_selfies_encodings_for_dataset(smiles_list, largest_smiles_len)\n",
    "# load the modedl\n",
    "vae_encoder, vae_decoder = load_models(4999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all the selfies strings to one-hot encoding\n",
    "batch_size = 1\n",
    "data = multiple_selfies_to_hot(selfies_list, largest_selfies_len,\n",
    "                                       selfies_alphabet)\n",
    "data = torch.tensor(data, dtype=torch.float).to(device)\n",
    "# Pick just the first molecule [C]\n",
    "batch = data[:batch_size]\n",
    "inp_flat_one_hot = batch.flatten(start_dim=1)\n",
    "# Encode it to a vector in latent space\n",
    "latent_points, mus, log_vars = vae_encoder(inp_flat_one_hot)\n",
    "latent_points = latent_points.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 11, 11, 11, 1, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 14, 11, 14]\n",
      "[Ring1][Branch2][nop][nop][nop][Ring1][nop][nop][nop][nop][nop][nop][nop][nop][nop][nop][nop][nop][=O][nop][=O]\n"
     ]
    }
   ],
   "source": [
    "gathered_atoms = []\n",
    "hidden = vae_decoder.init_hidden(batch_size=batch_size)\n",
    "# Decode the vector one position at a time\n",
    "for seq_index in range(batch.shape[1]):\n",
    "    out_one_hot, hidden = vae_decoder(latent_points, hidden)\n",
    "    out_one_hot = out_one_hot.flatten().detach()\n",
    "    soft = nn.Softmax(0)\n",
    "    out_one_hot = soft(out_one_hot)\n",
    "    out_index = out_one_hot.argmax(0)\n",
    "    gathered_atoms.append(out_index.data.cpu().tolist())\n",
    "print(gathered_atoms)\n",
    "print(\"\".join([selfies_alphabet[idx] for idx in gathered_atoms]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
